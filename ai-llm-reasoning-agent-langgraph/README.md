# ğŸ¤– AI LLM Reasoning Agent using LangGraph

An enterprise-style **Agentic AI system** built using **LangGraph + LangChain + Gemini LLM**, capable of **intelligent decision-making between RAG and direct LLM reasoning**, with **explainability and retrieval confidence scoring**.

This project demonstrates how modern GenAI systems go beyond simple chatbots by introducing **reasoning-based routing**, **grounded responses**, and **transparent decision logic**.

---

## ğŸš€ Key Features

âœ… LLM-based reasoning agent (LangGraph)  
âœ… Hybrid routing: **RAG vs Direct LLM**  
âœ… Vector similarityâ€“based confidence scoring  
âœ… Explainable AI decisions (`reason` field)  
âœ… FAISS vector database  
âœ… HuggingFace sentence-transformer embeddings  
âœ… FastAPI backend with interactive UI  
âœ… Google Gemini support (AI Studio & Vertex AI)  
âœ… Dockerized and deployed on Google Cloud Run  

---

## ğŸ§  What makes this Agentic AI?

Unlike traditional chatbots or basic RAG pipelines, this system:

- **Reasons before answering**
- **Decides the source of truth**
- **Explains why a route was chosen**
- **Shows confidence when using internal documents**

### Decision flow:
```
User Question
â†“
Reasoning Agent (LangGraph)
â†“
Evaluate relevance of internal knowledge
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If internal docs relevant â”‚
â”‚ â†’ RAG Path â”‚
â”‚ â€¢ FAISS Retrieval â”‚
â”‚ â€¢ Grounded Answer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Otherwise â”‚
â”‚ â†’ LLM Path â”‚
â”‚ â€¢ Gemini Reasoning â”‚
â”‚ â€¢ General Knowledge â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
Final Answer + Reason + Confidence
```

---

## ğŸ§© Architecture
```
FastAPI UI
â”‚
â–¼
LangGraph Reasoning Node
â”‚
â”œâ”€â”€ Vector similarity check
â”‚
â”œâ”€â”€ LLM-based routing decision
â”‚
â–¼
Conditional Execution
â”œâ”€â”€ RAG Node (FAISS + Docs)
â””â”€â”€ LLM Node (Gemini)
```

---

## ğŸ”€ Routing Logic

### 1ï¸âƒ£ Vector-based decision
- Uses FAISS similarity score
- High similarity â†’ internal documentation likely relevant

### 2ï¸âƒ£ LLM-based reasoning
If similarity is unclear, Gemini LLM decides:

```json
{
  "action": "rag | llm",
  "reason": "why this route is chosen"
}
```
This makes the system interpretable and explainable.
---

## ğŸ“Š Retrieval Confidence Scoring
When RAG is used:  
- FAISS distance is normalized into 0â€“1 confidence  
- Displayed as:  
    ğŸŸ¢ High confidence  
    ğŸŸ¡ Medium confidence  
    ğŸ”´ Low confidence
  
This helps users understand trust level of the answer.  

## ğŸ–¥ï¸ UI Preview
The FastAPI UI shows:
- Answer
- Route used (RAG / LLM)
- Retrieval confidence
- Reasoning explanation

This is ideal for enterprise demos and PoCs.

---
## ğŸ“ Project Structure
```
ai-llm-reasoning-agent-langgraph/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gcloudignore
â”‚
â””â”€â”€ docs/
    â””â”€â”€ sample.txt
```
## âš™ï¸ Tech Stack
- Python 3.11
- FastAPI
- LangGraph
- LangChain
- Google Gemini (2.5 Flash / Flash Lite)
- FAISS
- HuggingFace Embeddings
- Docker
- Google Cloud Run

## â–¶ï¸ Run Locally
### 1ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate

### 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

### 3ï¸âƒ£ Create .env
```
GOOGLE_API_KEY=your_api_key
USE_VERTEX=false
```
(For Vertex AI)
```
USE_VERTEX=true
GCP_PROJECT_ID=your_project_id
LOCATION=asia-south1
```
### 4ï¸âƒ£ Run application
uvicorn main:app --reload
Open:
http://localhost:8000

### ğŸ³ Run with Docker
docker build -t ai-llm-agent .
docker run -p 8080:8080 ai-llm-agent

## â˜ï¸ Cloud Deployment Overview (Google Cloud Run)
This application has been deployed using **Google Cloud Run** with the following setup:

- Docker-based containerization
- Stateless FastAPI service
- Gemini integration via:
  - Google AI Studio API key
  - Vertex AI (production-ready option)
- Environment-based configuration using `.env`

The same codebase supports both **local development** and **cloud deployment** without modification.

> Note: Public deployment URLs are intentionally not included to avoid dependency on runtime availability and cloud billing.

### ğŸ¯ Use Cases
- Enterprise knowledge assistants
- Internal policy Q&A systems
- Agentic GenAI PoCs
- Explainable RAG systems
- AI architecture demonstrations

### ğŸ“Œ Key Learning Outcomes
- How to build LLM reasoning agents
- LangGraph conditional routing
- Hybrid RAG architectures
- Confidence calibration
- Explainable GenAI design
- Production-ready GenAI deployment

### ğŸ‘¨â€ğŸ’» Author
Santhosh Gaddam
