
# ğŸ¤– LLM Reasoning Agent using LangGraph

An enterprise-style **LLM-powered reasoning agent** built using **LangGraph + LangChain + Gemini**, capable of making **intelligent routing decisions between RAG and direct LLM responses**, with full **explainability and confidence scoring**.

This project demonstrates how modern GenAI systems move beyond simple chatbots into **agentic architectures driven by LLM reasoning**.

---

## ğŸš€ Key Features

âœ… LLM-based reasoning for routing decisions  
âœ… LangGraph-based agent orchestration  
âœ… Hybrid execution: RAG vs direct LLM  
âœ… Explainable decision making  
âœ… Retrieval confidence scoring  
âœ… FAISS vector database  
âœ… HuggingFace sentence-transformer embeddings  
âœ… FastAPI backend with interactive UI  
âœ… Dockerized and Cloud Run compatible  

---

## ğŸ§  Agent Type

This project implements an **LLM reasoning agent**.

Routing decisions are made **by the LLM itself**, not by fixed rules or thresholds.

The LLM determines:

- whether internal documents are required
- whether general reasoning is sufficient
- why a particular route was chosen

LangGraph is used to **orchestrate and safely execute** the decision made by the LLM.

---

## ğŸ”€ Decision Flow
```
User Question
â†“
LLM-based Reasoning & Routing Decision
â†“
Agent Orchestration Layer (LangGraph)
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ If internal docs relevant â”‚
â”‚ â†’ RAG Path â”‚
â”‚ â€¢ FAISS Retrieval â”‚
â”‚ â€¢ Grounded Answer â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Otherwise â”‚
â”‚ â†’ LLM Path â”‚
â”‚ â€¢ Gemini Reasoning â”‚
â”‚ â€¢ General Knowledge â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
Final Answer + Reason + Confidence
```

---

## ğŸ“Š Retrieval Confidence

When RAG is selected, vector similarity scores are normalized into a **0â€“1 confidence score**, providing transparency into how strongly the retrieved documents support the answer.

Confidence levels:

- ğŸŸ¢ High
- ğŸŸ¡ Medium
- ğŸ”´ Low

---

## ğŸ§© Architecture Overview
```
FastAPI UI
â†“
LLM Reasoning Node
â†“
LangGraph State Machine
â†“
Conditional Execution
â”œâ”€â”€ RAG Node (FAISS + Docs)
â””â”€â”€ LLM Node (Gemini)
```

---

## ğŸ“ Project Structure
```
llm-reasoning-agent-langgraph/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gcloudignore
â”‚
â””â”€â”€ docs/
â””â”€â”€ sample.txt
```


---

## âš™ï¸ Tech Stack

- Python 3.11
- FastAPI
- LangGraph
- LangChain
- Google Gemini (Flash / Flash Lite)
- FAISS
- HuggingFace Embeddings
- Docker

---

## â–¶ï¸ Run Locally

```
pip install -r requirements.txt
uvicorn main:app --reload
```
## ğŸ³ Docker
```
d111ocker build -t llm-reasoning-agent .
docker run -p 8080:8080 llm-reasoning-agent
```

## â˜ï¸ Deployment Overview
This application has been deployed using Google Cloud Run with Docker-based containerization.  
The same codebase supports both:  
- Google AI Studio API Key
- Vertex AI (production-ready)  
Deployment URLs are intentionally excluded to avoid runtime dependency and billing exposure.

##  ğŸ¯ Key Learnings
- Difference between deterministic routing and LLM reasoning  
- Agent orchestration using LangGraph
- Explainable GenAI system design
- Hybrid RAG architectures
- Production-ready GenAI deployment

ğŸ‘¨â€ğŸ’» Author
Santhosh Gaddam
